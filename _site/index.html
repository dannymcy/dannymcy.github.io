<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Chenyang (Danny) Ma | University of Oxford</title>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="The Minimal Light is a simple and elegant jekyll theme for academic personal homepage.">
    
    <meta name="keywords" content="minimal light">
    
    
    <link rel="canonical" href="https://minimal-light-theme.yliu.me/"/>
    

    <link rel="icon" media="(prefers-color-scheme:dark)" href="./assets/img/favicon-dark.png" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/img/favicon.png" type="image/png" />
    <script src="./assets/js/favicon-switcher.js" type="application/javascript"></script>

    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous>

    <link rel="stylesheet" href="./assets/css/style.css">
    <link rel="stylesheet" href="./assets/css/publications.css">

  </head>
  <body>
    <div class="wrapper">
      <header>
        
        
        <a class="image avatar"><img src="./assets/img/avatar.jpg" alt="avatar" /></a>
        

        <h1>Chenyang (Danny) Ma</h1>

        
        <position style="font-size:1.10rem;">PhD Student</position>
        <br>
        
        
        <a href="" rel="noopener"><autocolor>University of Oxford</autocolor></a>
        <br>
        
        
        <email>chenyang.ma [at] cs.ox.ac.uk</email>
        

        <br>
        <br>
        <div class="social-icons">
        
        <a style="margin: 0 5px 0 0" href="https://scholar.google.com/citations?user=d4tuNoUAAAAJ&hl=en">
          <i class="ai ai-google-scholar" style="font-size:1.2rem"></i>
        </a>  
        

        
        <a style="margin: 0 5px 0 0" href="assets/files/Chenyang_Ma_Resume.pdf">
          <i class="ai ai-cv" style="font-size:1.3rem;"></i>
        </a>
        

        
        <a style="margin: 0 5px 0 0" href="https://github.com/dannymcy">
          <i class="fab fa-github"></i>
        </a>
        

        
        <a style="margin: 0 5px 0 0" href="https://www.linkedin.com/in/chenyang-ma-66945091/">
          <i class="fab fa-linkedin"></i>
        </a>
        

        
        </div>
        <br>

      </header>
      <section>

      <h2 id="about-me">About Me</h2>

<p>I am a Ph.D. student in Computer Science at the <a href="https://www.ox.ac.uk">University of Oxford</a>, co-supervised by Prof. <a href="https://www.cs.ox.ac.uk/people/andrew.markham/">Andrew Markham</a> and Prof. <a href="https://en.wikipedia.org/wiki/Niki_Trigoni">Niki Trigoni</a>. My study is generously funded by the EPSRC DTP International Doctoral Scholar scholarship. I had the privilege of collaborating closely with <a href="https://www.xavierpuigf.com/">Xavier Puig</a> and <a href="https://rutadesai.github.io/">Ruta Desai</a> at <a href="https://ai.meta.com/research/#fundamental-and-applied">Meta FAIR</a> (2024) and spent wonderful times interning at <a href="https://www.roku.com/en-gb/">Roku</a> (2024) and <a href="https://flower.ai/">Flower Labs</a> (2023).</p>

<p>Prior to my Ph.D., I worked as a research assistant in <a href="https://mlsys.cst.cam.ac.uk/">Cambridge Machine Learning Systems Lab</a>, supervised by Prof. <a href="https://niclane.org/">Nicholas Lane</a>. I obtained my B.S.E. degree from <a href="https://umich.edu/">University of Michigan</a>, completing multiple projects under the guidances of Prof. <a href="https://andrewowens.com/">Andrew Owens</a>, Prof. <a href="http://robotouch.ri.cmu.edu/yuanwz/">Wenzhen Yuan</a>, and Prof. <a href="https://www.umtri.umich.edu/people/green-paul-a/">Paul Green</a>.</p>

<p style="color:red;">I am actively looking for a research internship for Summer 2025. Feel free to contact me for any research opportunities!</p>

<p style="margin-top: 40px;"></p>

<h2 id="research-interests">Research Interests</h2>

<p>My main research interest focuses on developing human-centered intelligent agents that live in both virtual and physical worlds. So far, my Ph.D. work falls in the intersection of 3D Understanding, Robot Learning, and Computer Vision.</p>

<!-- My Ph.D. work mainly builds on two closely-related questions: -->

<!-- 1. Can we generate datasets of 3D shape with high fidelity and establish pose correspondences from large-scale in-the-wild 2D images?
2. Can we use features from 3D space to enhance the capabilities of image generative models? -->

<p>On the side, I also enjoy working with vision applications that utilize various modalities (Vision, Tactile) and AI methods that are less centralized and more collaborative (Federated Learning).</p>

<p style="margin-top: 40px;"></p>

<h2 id="news">News</h2>
<!-- - **[Feb. 2024]** Our paper Continuous 3D Words got accepted into CVPR 2024! Check out our work [here](http://ttchengab.github.io/continuous_3d_words/)!
- **[Sep. 2023]** Our paper on Multi-Body SE(3) Equivariance is accepted to NeurIPS 2023!
- **[Aug. 2023]** Three papers are accepted to IEEE Transactions on Intelligent Transportation Systems, WACV 2024, and BMVC 2024.
- **[Jul. 2023]** Our paper 3DMiner got accepted into ICCV 2023!
- **[May. 2023]** Started my second internship at Adobe Research! (Mentors: [Matheus Gadelha](http://mgadelha.me), [Thibault Groueix](https://imagine.enpc.fr/~groueixt/), [Matthew Fisher](https://techmatt.github.io), and [Radomir Mech](https://research.adobe.com/person/radomir-mech/))
- **[Dec. 2022]** Defended my [Transfer of Status](https://www.ox.ac.uk/students/academic/guidance/graduate/research/status/DPhil) viva (Examiners: [Prof. Yarin Gal](https://www.cs.ox.ac.uk/people/yarin.gal/website/), [Prof. Ronald Clark](https://www.ron-clark.com)). -->
<ul>
  <li><strong>[Sep. 2024]</strong> Our paper SpatialPIN got accepted into NeurIPS 2024!</li>
  <li><strong>[Jul. 2024]</strong> Started my internship at Roku Advanced Development! (Mentor: <a href="http://linkedin.com/in/michael-sanders-8b52b1164">Michael Sanders</a>)</li>
  <li><strong>[Jun. 2024]</strong> Started my research collaboration with Meta FAIR! (Mentors: <a href="https://www.xavierpuigf.com/">Xavier Puig</a>, <a href="https://rutadesai.github.io/">Ruta Desai</a>)</li>
  <li><strong>[Oct. 2023]</strong> Started my Ph.D. journey at the University of Oxford.</li>
</ul>

<p style="margin-top: 40px;"></p>
<h2 id="publications" style="margin: 2px 0px -15px;">Selected Publications <span style="font-size:14px;">(Full list on <a href="https://scholar.google.com/citations?user=d4tuNoUAAAAJ&amp;hl=en">Google Scholar</a>)</span></h2>

<div class="publications">
<ol class="bibliography">

<li>
<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/paper_teasers/spatialpin.png" class="teaser img-fluid z-depth-1" />
    <abbr class="badge">NeurIPS</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title">SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors</div>
    <div class="author"><strong>Chenyang Ma</strong>, Kai Lu, Ta-Ying Cheng, Niki Trigoni, Andrew Markham</div>
    <div class="periodical"><em><strong>Neural Information Processing Systems (NeurIPS)</strong>, 2024.</em></div>
    <div class="links">
      <a href="https://dannymcy.github.io/zeroshot_task_hallucination/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://arxiv.org/abs/2403.13438" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
      <a href="https://github.com/dannymcy/zeroshot_task_hallucination_code" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <!-- <a href="https://arxiv.org/pdf/2002.10211.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/yaoyao-liu/mnemonics" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://class-il.mpi-inf.mpg.de/mnemonics/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://dblp.uni-trier.de/rec/conf/cvpr/LiuSLSS20.html?view=bibtex" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      <strong><i style="color:#e74d3c">Oral Presentation</i></strong> -->
    </div>
  </div>

</div>
<br />
  

<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/paper_teasers/fedxgbllr.png" class="teaser img-fluid z-depth-1" />
    <abbr class="badge">EuroMLSys</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title">Gradient-less Federated Gradient Boosting Tree with Learnable Learning Rates</div>
    <div class="author"><strong>Chenyang Ma</strong>, Xinchi Qiu, Daniel Beutel, Nicholas Lane</div>
    <div class="periodical"><em><strong>Workshop on Machine Learning and Systems (EuroMLSys)</strong>, 2023.</em></div>
    <div class="links">
      <a href="https://flower.ai/blog/2023-04-19-xgboost-with-flower/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://arxiv.org/abs/2304.07537" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
      <a href="https://github.com/adap/flower/tree/main/baselines" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <!-- <a href="https://arxiv.org/pdf/2002.10211.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/yaoyao-liu/mnemonics" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://class-il.mpi-inf.mpg.de/mnemonics/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://dblp.uni-trier.de/rec/conf/cvpr/LiuSLSS20.html?view=bibtex" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      <strong><i style="color:#e74d3c">Oral Presentation</i></strong> -->
    </div>
  </div>

</div>
<br />
  

<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/paper_teasers/touch_go.png" class="teaser img-fluid z-depth-1" />
    <abbr class="badge">NeurIPS</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title">Touch and Go: Learning from Human-Collected Vision and Touch</div>
    <div class="author">Fengyu Yang*, <strong>Chenyang Ma*</strong>, Jiacheng Zhang, Jing Zhu, Wenzhen Yuan, Andrew Owens (*=Equal Contribution)
    </div>
    <div class="periodical"><em><strong>NeurIPS Datasets and Benchmarks</strong>, 2022.</em></div>
    <div class="links">
      <a href="https://touch-and-go.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://arxiv.org/abs/2211.12498" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
      <a href="https://drive.google.com/drive/u/1/folders/1NDasyshDCL9aaQzxjn_-Q5MBURRT360B" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Dataset</a>
        <a href="https://github.com/fredfyyang/Touch-and-Go" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <!-- <a href="https://arxiv.org/pdf/2002.10211.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">PDF</a>
      <a href="https://github.com/yaoyao-liu/mnemonics" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Code</a>
      <a href="https://class-il.mpi-inf.mpg.de/mnemonics/" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Project Page</a>
      <a href="https://dblp.uni-trier.de/rec/conf/cvpr/LiuSLSS20.html?view=bibtex" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">BibTex</a>
      <strong><i style="color:#e74d3c">Oral Presentation</i></strong> -->
    </div>
  </div>

</div>
<br />


<div class="pub-row">

  <div class="col-sm-3 abbr" style="position: relative;padding-right: 15px;padding-left: 15px;">
    <img src="assets/img/paper_teasers/geospatial.png" class="teaser img-fluid z-depth-1" />
    <abbr class="badge">CVPR</abbr>
  </div>

  <div class="col-sm-9" style="position: relative;padding-right: 15px;padding-left: 20px;">
    <div class="title">Sparse and Complete Latent Organization for Geospatial Semantic Segmentation</div>
    <div class="author">Fengyu Yang*, <strong>Chenyang Ma*</strong> (*=Equal Contribution)
    </div>
    <div class="periodical"><em><strong>Computer Vision and Patter Recognition (CVPR)</strong>, 2022.</em></div>
    <div class="links">
      <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Sparse_and_Complete_Latent_Organization_for_Geospatial_Semantic_Segmentation_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" style="font-size:12px;">Paper</a>
      <!-- <strong><i style="color:#e74d3c">Oral Presentation</i></strong> -->
    </div>
  </div>

</div>



</li>

<br />

</ol>
</div>

<p style="margin-top: 40px;"></p>
<h2 id="services">Services</h2>

<h4 style="margin:0 10px 0;">Conference Reviewer</h4>

<ul style="margin:0 0 5px;">
  <li><a href="http://cvpr2023.thecvf.com/"><autocolor>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</autocolor></a></li>
  <li><a href="https://neurips.cc/Conferences/2022/CallForDatasetsBenchmarks"><autocolor>NeurIPS Datasets and Benchmarks Track 2022</autocolor></a></li>
</ul>

<h4 style="margin:0 10px 0;">Teaching Assistant</h4>

<ul style="margin:0 0 5px;">
  <li><a href="https://www.cs.ox.ac.uk/teaching/courses/2023-2024/ml/"><autocolor>Machine Learning</autocolor></a></li>
  <li><a href="https://www.cs.ox.ac.uk/teaching/courses/2023-2024/dlhealthcare/"><autocolor>Deep Learning in Healthcare</autocolor></a></li>
</ul>

<!-- <h4 style="margin:0 10px 0;">Journal Reviewers</h4>

<ul style="margin:0 0 20px;">
  <li><a href="https://www.computer.org/csdl/journal/tp"><autocolor>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</autocolor></a></li>
  <li><a href="https://www.springer.com/journal/11263"><autocolor>International Journal of Computer Vision (IJCV)</autocolor></a></li>
</ul> -->



      <br>

      
      <p><small>Powered by Jekyll and <a href="https://github.com/yaoyao-liu/minimal-light" target="_blank" rel="noopener">Minimal Light</a> theme.</small></p>
      

      </section>
      <footer>
        
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
